{
 "cells": [
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réalisé par :\n",
    "AIT AMARA Mohamed, 181831072170\n",
    "\n",
    "BOUROUINA Rania, 181831052716\n",
    "\n",
    "CHIBANE Ilies, 181831072041\n",
    "\n",
    "HAMMAL Ayoub, 181831048403"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fd495f",
   "metadata": {},
   "source": [
    "# Rapport du TP7 :  Machine à Vecteur de Support (MVS)\n",
    "\n",
    "Le but de ce rapport est d'expliquer le fonctionnement des Machine à Vecteur de Support (Support Vector Machine ou SVM), leurs entrées et leurs sorties.\n",
    "\n",
    "## Intuition\n",
    "\n",
    "Le but des MVS est de trouver une fonction de séparation linéaire dont le signe servira de fonction de classification : $$sign(W^{T} X + b) \\rightarrow \\{0, 1\\}$$\n",
    "\n",
    "Pour cela, nous aimerons maximizer la largeur de la marge séparatrice entre les deux classes, pour maximiser ainsi la confiance du classifieur.\n",
    "\n",
    "## Démonstration\n",
    "\n",
    "### MVS à marge stricte \n",
    "\n",
    "La formule du hyperplan séparateur est : $h = W^{T} X + b$.\n",
    "\n",
    "Une solution pour ce problème est la suivante : \n",
    "\n",
    "Si $W^{T} X + b \\geq 0 \\implies \\text{X est +}$\n",
    "\n",
    "Sinon si $W^{T} X + b < 0 \\implies \\text{X est -}$\n",
    "\n",
    "Cependant, cette règle de décision : $(\\delta) W^{T} X + b = 0$ est que même $k W, k b$ sont des solutions pour l'equation. Par conséquent, en pratique la règles suivante est employée : \n",
    "\n",
    "Pour chaque X de classe + : $W^{T} X + b \\geq 1$\n",
    "\n",
    "Pour chaque X de classe - : $W^{T} X + b \\leq -1$\n",
    "\n",
    "Et on a $y_{i} =\n",
    "\\begin{cases}\n",
    "    1 & \\quad \\text{Pour X +}\\\\\n",
    "    -1 & \\quad \\text{Pour X -}\n",
    "\\end{cases}$\n",
    "\n",
    "La règle peut donc être simplifiée comme suit:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& y_{i}(W^{T} X_{i} + b) - 1 \\geq 0 \\nonumber \\\\\n",
    "& \\text{et} \\nonumber \\\\\n",
    "& y_{i}(W^{T} X_{i} + b) - 1 = 0 \\quad \\text{ pour chaque } X_{i} \\text{ sur la frontière de décision (les marges)} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Pour que l'hyperplan obtenu soit optimal, la largeur de la frontière de séparation doit être maximale. Les vecteurs de support qui sont les points situés sur la bordure de la frontière vont nous aider pour cela.\n",
    "\n",
    "<img src=\"svm_diagram.png\" align =\"center\"/>\n",
    "\n",
    "Le vecteur $\\overrightarrow{W}$ représente les paramètres de notre MVS et par la même occasion la norme de l'hyperplan séparateur. Vu qu'il est perpendiculaire à la frontière de séparation, nous l'utilisons pour calculer la largeur de la frontière comme suit :\n",
    "\n",
    "Sachant que pour $X^{+}$ et $X^{-}$ sur les marges de la frontière de décision :\n",
    "$$y_{i}(W^{T} X^{+}_{i} + b) - 1 = 0 \\quad \\text{ et } \\quad y_{i}(W^{T} X^{-}_{i} + b) - 1 = 0$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "width & = (\\overrightarrow{X^{+}} - \\overrightarrow{X^{-}}) . \\frac{\\overrightarrow{W}}{\\|\\overrightarrow{W}\\|_{2}} \\quad \\text{(produit vectoriel)} \\nonumber \\\\\n",
    "      & = \\frac{(1 - b) - (-1 - b)}{\\overrightarrow{W}} . \\frac{\\overrightarrow{W}}{\\|\\overrightarrow{W}\\|_{2}} \\nonumber \\\\\n",
    "      & = \\frac{2}{\\|\\overrightarrow{W}\\|_{2}} \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Pour faciliter la dérivation et les étapes suivantes, nous allons au lieu de maximiser la largeur, minimiser $\\frac{1}{2}\\|\\overrightarrow{W}\\|_{2}$.\n",
    "\n",
    "Pour résumer, nous devons trouver la solution pour le groupe d'équations suivant :\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\text{min } & \\quad \\frac{1}{2}\\|\\overrightarrow{W}\\|_{2} \\\\\n",
    "    \\text{sachant que } & \\quad y_{i}(W^{T} X_{i} + b) - 1 \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Qui est un problème d'optimisation quadratique qu'on résout avec le _Multiplicateur de Lagrange_ (problème Dual).\n",
    "\n",
    "### MVS à marge souple\n",
    "\n",
    "L'approche d'optimisation précédent aboutit à une solution seulement si les points sont linéairement séparables. Pour pallier à cet inconvénient, nous introduisons dans ce qui suit les MVS à marge souple.\n",
    "\n",
    "Nous permettons la violation des marges avec l'introduction des variables ressort $\\epsilon_{i}$ (slack variables), les formules précédentes deviennent :\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    \\text{min } & \\quad \\frac{1}{2} W^{T} W + C \\sum_{i=1}^{n} \\epsilon_{i}  \\quad \\text{ , C est un hyperparamètre}\\\\\n",
    "    \\text{sachant que } & \\quad y_{i}(W^{T} X_{i} + b) \\geq 1 - \\epsilon_{i}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "La variable ressort permet aux $X_{i}$ de franchir le marge et de même être sur le mauvais côté. Le $C$ permet de controler le coût d'un tel compromis, si $C$ est très grand le MVS devient strict, sinon, avec un petit $C$, il sacrifie des points pour obtenir une solution simple.\n",
    "\n",
    "Une solution typique pour fixer les variables ressort est la suivante :\n",
    "\n",
    "$$\n",
    "\\epsilon_{i} = \n",
    "\\begin{cases}\n",
    "    1 - y_{i}(W^{T} X_{i} + b) & \\quad \\text{ si } y_{i}(W^{T} X_{i} + b) < 1 \\\\\n",
    "    0 & \\quad \\text{ si } y_{i}(W^{T} X_{i} + b) \\geq 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Cette solution vérifie la deuxième condition ($1 \\geq 1$), dont il ne resque qu'à minimiser la première formule qui devient :\n",
    "\n",
    "$$ \\text{min} \\quad \\underbrace{\\frac{1}{2} W^{T} W}_\\text{L2 régularisation}+ \\underbrace{C \\sum_{i=1}^{n} max(1 - y_{i}(W^{T} X_{i} + b), 0)}_\\text{Hinge loss} $$\n",
    "\n",
    "On peut utiliser encore l'approche Dual, ou bien l'approche de résolution Primal (descente de gradient) pour résoudre cette formule.\n",
    "\n",
    "## Kernels\n",
    "\n",
    "Les fonctions kernels sont utilisées plus souvent avec la méthode Dual, mais il existe une méthode pour appliquer <a href=\"https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.129.3368&rep=rep1&type=pdf\">les kernels avec la méthode de résolution Primal</a>.\n",
    "\n",
    "> Soit $\\beta$ les paramètres dans le nouvel espace, et $K$ la matrice kernel telle que $K_{ij} = K(x_{i}, x_{j})$\n",
    ">\n",
    "> La dernière formule devient ($\\lambda = 1/C$ et L est la fonction de Hinge Loss):\n",
    ">\n",
    "> $$ \\text{min} \\quad \\lambda \\beta^{T} K \\beta + \\sum_{i=1}^{n} L(y_{i}, K_{i}^{T} \\beta)$$\n",
    "\n",
    "Une fonction kernel $\\phi$ permet d'appliquer une transformation sur l'espace d'entrée. Les plus connues sont les fonctions de kernel _polynomiale_ et _radiale_.\n",
    "\n",
    "Pour le noyau polynomial, nous devons préciser la dimension de sortie : \n",
    "\n",
    "$$\n",
    "\\phi(X) = \\phi(\\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix}) = \\begin{pmatrix} x_{1}^{2} \\\\ \\sqrt{2}x_{1}x_{2} \\\\ x_{2}^{2} \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Cependant le kernel polynomial prend deux points et calcul directement le produit $\\phi(a)^{T} \\phi(b)$ qui constitue une partie de la solution du problème d'optimisation quadratique présenté précédemment. Sa formule après simplification est la suivante (exemple de polynome de degré 2) : \n",
    "\n",
    "$$\n",
    "K(a, b) = (\\gamma a^{T} b + r)^{d} \\quad \\text{ ou $r$ est le terme indépendent et $d$ est le degré du polynome}\n",
    "$$\n",
    "\n",
    "Pour le kernel radial (radial basis function or rbf), sa fonction est la suivante :\n",
    "\n",
    "$$\n",
    "K(a, b) = exp(-\\gamma \\|a - b\\|^{2})\n",
    "$$\n",
    "\n",
    "Le terme $\\gamma$ définit l'influence qu'a chaque point sur le reste des autres points. Plus il est grand plus la projection d'un point dépend des points adjacents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
